{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data visualisation\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "# to scale data and possible reduce dimensionality of data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for data modelling \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('application_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100* df.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = pd.DataFrame(100* df.isnull().sum()/len(df)).rename(columns={0:'%_Missing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing[percent_missing['%_Missing']>10].count()\n",
    "# i will remove any columns with missing values > 10% to simplify the cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will attempt to get the names of the 49 columns to remove from the dataset\n",
    "drop_cols = percent_missing[percent_missing['%_Missing']>10].T.columns.tolist()\n",
    "\n",
    "# then drop them from the dataset\n",
    "for cols in drop_cols:\n",
    "    df.drop([cols],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100* df.isnull().sum()/len(df)\n",
    "# i will remove any records with null values since missing values are less than 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame(100* df.isnull().sum()/len(df)).rename(columns={0:'%_NA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_na = b[b['%_NA']>0].T.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=drop_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df,x='TARGET',y='AMT_CREDIT',title='Box Plot of credit amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()\n",
    "# IQR is tighter for those with payment difficulties \n",
    "# ($284.4k ~ $737.5k v.s $270k ~ 810k)\n",
    "# interesting that max loan amount is similar at ~$4mil\n",
    "# logic might be that banks are only willing loan smaller amounts due to risk of defaulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annuity in this context is the amount the applicant has to repay on a regular basis to repay the loan\n",
    "fig = px.box(df,x='TARGET',y='AMT_ANNUITY',title='Box Plot of annuity amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()\n",
    "# the graph shows that those with troubles repaying are forced into repaying more \n",
    "# each time in an attempt by the back to recoup on risky loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"NAME_CONTRACT_TYPE\", col=\"TARGET\", data=df, kind=\"count\", height=4, aspect=.7)\n",
    "# more cash loans are giving than revolving loans, loans applicants do not have anything valuable to act as a collateral?\n",
    "sns.catplot(x=\"CODE_GENDER\", col=\"TARGET\", data=df, kind=\"count\", height=4, aspect=.7)\n",
    "# more loans more given to females than males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"FLAG_OWN_REALTY\", col=\"TARGET\", data=df, kind=\"count\", height=4, aspect=.7)\n",
    "# more \n",
    "sns.catplot(x=\"FLAG_OWN_CAR\", col=\"TARGET\", data=df, kind=\"count\", height=4, aspect=.7)\n",
    "# more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"REG_CITY_NOT_LIVE_CITY\", col=\"TARGET\", data=df, kind=\"count\", height=4, aspect=.7)\n",
    "sns.catplot(x=\"REG_CITY_NOT_WORK_CITY\", col=\"TARGET\", data=df, kind=\"count\", height=4, aspect=.7)\n",
    "# seems like a larger portion of defaulters do not live/work in the same city as where they applied for their loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"NAME_HOUSING_TYPE\", col=\"TARGET\", data=df, kind=\"count\", height=5, aspect=1.9)\n",
    "# a rare rule might be that if someone lives in a rented, municipal, office or co-op apartment\n",
    "# they are very unlikely to default on payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"NAME_EDUCATION_TYPE\", col=\"TARGET\", data=df, kind=\"count\", height=5, aspect=1.9)\n",
    "# seems like those with/undergoing higher education + degrees rarely take loans and rarely default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(x='AMT_INCOME_TOTAL', hue='TARGET',data=df, bins=50, log_scale=True,color='Blue',binwidth=.1)\n",
    "# there seems to be an extreme record for income, why would someone with so much income need a loan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['AMT_INCOME_TOTAL']==df['AMT_INCOME_TOTAL'].max()]\n",
    "# $117,000,000! it might be a typo, best to remove it\n",
    "df = df[df['AMT_INCOME_TOTAL'] != 117000000.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['SK_ID_CURR'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning of binary variables with one-hot encoding\n",
    "\n",
    "df['NAME_CONTRACT_TYPE'] = df['NAME_CONTRACT_TYPE'].replace({'Cash loans':1,'Revolving loans':0})\n",
    "\n",
    "df['CODE_GENDER'] = df['CODE_GENDER'].replace({'XNA':'F'})\n",
    "df['CODE_GENDER'] = df['CODE_GENDER'].replace({'F':'1','M':0})\n",
    "\n",
    "df['FLAG_OWN_CAR'] = df['FLAG_OWN_CAR'].replace({'Y':1,'N':0})\n",
    "\n",
    "df['FLAG_OWN_REALTY'] = df['FLAG_OWN_REALTY'].replace({'Y':1,'N':0}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning of the age and days employed and converting it into years and removing negative sign\n",
    "df['AGE'] = df['DAYS_BIRTH']/-365\n",
    "df['EMP_LENGTH'] = df['DAYS_EMPLOYED']/-365\n",
    "df.drop(['DAYS_BIRTH','DAYS_EMPLOYED'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to be used for Kmeans modelling \n",
    "col = [\n",
    "    # numerical variables\n",
    "    'TARGET', 'CODE_GENDER', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n",
    "    'REGION_POPULATION_RELATIVE', 'AGE', 'EMP_LENGTH', 'REGION_RATING_CLIENT', 'EXT_SOURCE_2',\n",
    "    # categorical variables\n",
    "    'NAME_CONTRACT_TYPE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE'\n",
    "]\n",
    "df_model = df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a heat map for feature engineering\n",
    "model_corr = df_model.corr()\n",
    "mask = np.triu(np.ones_like(model_corr, dtype=np.bool))\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "# Draw the heatmap with 'sns.heatmap()'\n",
    "ax= sns.heatmap(model_corr, mask=mask, annot=True, square=True, linewidth=0.5, vmin=-1, vmax=1, cmap='coolwarm',fmt = '.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new variables to reduce dimensionality .\n",
    "\n",
    "# NUM_PAYMENT = ( [ANNUITY] / [CREDIT] )\n",
    "# the greater the ratio, the greater the chance to default due to perceived diffculty \n",
    "# in recouping principle amount from applicant\n",
    "df_model['ANNUITY_TO_CREDIT_RATIO'] = df_model['AMT_ANNUITY'] / df_model['AMT_CREDIT']\n",
    "df_model.drop(['AMT_CREDIT','AMT_ANNUITY'],axis=1,inplace=True)\n",
    "\n",
    "# NUM_PAYMENT = ( [EMP_LENGTH] / [AGE] )\n",
    "# the greater the ratio, the lower the chance to default due to a steady source of income for repayment\n",
    "# in recouping principle amount from applicant\n",
    "df_model['WORK_TO_AGE_RATIO'] = df_model['EMP_LENGTH'] / df_model['AGE']\n",
    "df_model.drop(['EMP_LENGTH','AGE'],axis=1,inplace=True)\n",
    "\n",
    "# RATING_TO_DENSITY_RATIO = ( [REGION_RATING_CLIENT] / [REGION_POPULATION_RELATIVE] )\n",
    "# the greater the REGION_RATING_CLIENT value, the more likely the applicant will can finanical diffculties with repayment\n",
    "# this infers that a lower regional rating score is better\n",
    "# the more populated the region the applicant lives in, the lower their regional rating score (desirable)\n",
    "# thus, the lower the ratio, the more desirabke it is to loan to the applicant\n",
    "df_model['RATING_TO_DENSITY_RATIO'] = df_model['REGION_RATING_CLIENT'] / df_model['REGION_POPULATION_RELATIVE']\n",
    "df_model.drop(['REGION_RATING_CLIENT','REGION_POPULATION_RELATIVE'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all categorical variables\n",
    "df_model_num = df_model.drop(['NAME_CONTRACT_TYPE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model_num.drop(['TARGET'], axis=1)\n",
    "y = df_model_num['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y,kmeans.labels_))\n",
    "print()\n",
    "print(classification_report(y,kmeans.labels_))\n",
    "\n",
    "# Output\n",
    "\n",
    "# [[237313  42551]\n",
    "#  [ 21678   2988]]\n",
    "\n",
    "#              precision    recall  f1-score   support\n",
    "#           0       0.92      0.85      0.88    279864\n",
    "#           1       0.07      0.12      0.09     24666\n",
    "\n",
    "#    accuracy                           0.79    304530\n",
    "#   macro avg       0.49      0.48      0.48    304530\n",
    "#weighted avg       0.85      0.79      0.82    304530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=101,n_components=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame(pca.components_,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='coolwarm',annot=True)\n",
    "# i will be ignoring any components with coefficient < 0.00 as it will be insignificant in interepting PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC1 : Perfect negative correlation with gender\n",
    "\n",
    "# PC2 and PC3 seems similar and contradicts\n",
    "\n",
    "# PC2 : Contrast btw the ext source 2 and relative high annuity:credit ratio\n",
    "# PC3 : Weight sum btw ext source 2 and relative higher annuity:credit ratio\n",
    "# PC4 : Perfect negative correlation with work:age ratio\n",
    "# PC5 : Perfect correlation with # of children\n",
    "# PC6 : Perfect correlation with rating:density ratio\n",
    "# PC7 : Perfect correlation with total income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.round(2)\n",
    "\n",
    "# coefficents for each PC\n",
    "\n",
    "#array([[-1.  ,  0.  ,  0.  , -0.01,  0.04,  0.05, -0.  ],\n",
    "#       [ 0.03,  0.  , -0.  , -0.84,  0.55, -0.01,  0.02],\n",
    "#       [ 0.03,  0.  ,  0.  ,  0.55,  0.84,  0.03, -0.01],\n",
    "#       [-0.05, -0.07, -0.  ,  0.02,  0.02, -1.  , -0.  ],\n",
    "#       [ 0.  ,  1.  , -0.  ,  0.01, -0.  , -0.07,  0.06],\n",
    "#       [-0.  , -0.06, -0.  ,  0.02, -0.  ,  0.  ,  1.  ],\n",
    "#       [ 0.  ,  0.  ,  1.  , -0.  ,  0.  , -0.  ,  0.01]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.round(2)\n",
    "# 94% of variance explained with the first 3 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=101,n_components = 3)\n",
    "df_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reset_index().drop(['index'],axis=1)\n",
    "df_pca_model = pd.DataFrame(df_pca, columns=[\"PCA1\", \"PCA2\", \"PCA3\"])\n",
    "#df_pca_model['TARGET'] = y\n",
    "#df_pca_model = df_pca_model.drop(['PCA4','PCA5'],axis=1)\n",
    "df_pca_model.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(random_state=101,n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(df_pca_model) #.drop('TARGET',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_\n",
    "\n",
    "#array([[ 0.65932912, -0.00461518, -0.00436472],\n",
    "#       [-0.34216973,  0.00239512,  0.00226514]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(confusion_matrix(y,kmeans.labels_))\n",
    "print(classification_report(y,kmeans.labels_))\n",
    "\n",
    "# Output\n",
    "#[[ 93441 186423]\n",
    "# [ 10604  14062]]\n",
    "\n",
    "#              precision    recall  f1-score   support\n",
    "#\n",
    "#           0       0.90      0.33      0.49    279864\n",
    "#           1       0.07      0.57      0.12     24666\n",
    "\n",
    "#    accuracy                           0.35    304530\n",
    "#   macro avg       0.48      0.45      0.31    304530\n",
    "#weighted avg       0.83      0.35      0.46    304530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_model['PRED_TARGET'] = kmeans.labels_\n",
    "df_pca_model['TARGET'] = y\n",
    "df_pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new model has made better attempts to correctly classify those with payment diffculties\n",
    "# but has classified more false positives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
